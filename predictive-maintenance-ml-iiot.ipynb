{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Dataset:\n    # Simulated engine degradation under different combinations of operational conditions and modes \n    # Records several sensor channels to characterize fault evolution. Provided by NASA\n    # https://ti.arc.nasa.gov/tech/dash/groups/pcoe/prognostic-data-repository/\n# Approach:\n    # Prognostics: predicting the time at which something will no longer perform it's indended function\n    # Remaining Useful Life (RUL): continious target, number of useful cycles left on the engine\n    # Last 15 Cycles: binary target, represents whether the asset is in the last 15 cycles of life\n    # This approach predicts 'RUL' and 'Last 15 Cycles' by treating each time point independently\n    # A next step is to spend time feature engineering\n    # A next step would be to use LSTM or another time series appropriate deep learning approach","metadata":{"_cell_guid":"796409f1-2151-4893-96b7-08d37b1ef2f1","_uuid":"b6f3f66702055d1230e2a35e34e1dd3f3a86e20e","collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# load necessary packages and view available data\nimport os\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport keras\n%matplotlib inline\nsns.set()\nprint(os.listdir(\"../input\"))\n# Setting seed for reproducability\nnp.random.seed(1234)  \nPYTHONHASHSEED = 0","metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"code","source":"# the files did not contain headers. Here we create labels based on documentation\ntarget_var = ['Target_Remaining_Useful_Life']\nindex_columns_names =  [\"UnitNumber\",\"Cycle\"]\nop_settings_columns = [\"Op_Setting_\"+str(i) for i in range(1,4)]\nsensor_columns =[\"Sensor_\"+str(i) for i in range(1,22)]\ncolumn_names = index_columns_names + op_settings_columns + sensor_columns\nprint(column_names)","metadata":{"_cell_guid":"ff6a0d12-c32d-4a0a-9984-59200fbe6042","_uuid":"4217a2b31bebe713fa99cb4c0dfd0e1c58660c67","scrolled":true,"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"code","source":"# load data\ntrain= pd.read_csv('../input/train_FD001.txt', sep=\" \", header=None)\ntest = pd.read_csv('../input/test_FD001.txt', sep=\" \", header=None)\nprint(\"train shape: \", train.shape, \"test shape: \", test.shape)\n# drop pesky NULL columns\ntrain.drop(train.columns[[26, 27]], axis=1, inplace=True)\ntest.drop(test.columns[[26, 27]], axis=1, inplace=True)\n# name columns\ntrain.columns = [column_names]\ntest.columns = [column_names]\ntrain[train['UnitNumber'] == 1].head(5)\ntest[test['UnitNumber'] == 1].head(5)","metadata":{"_cell_guid":"54f55c82-5f92-4174-849e-02978183af0f","_uuid":"1eb175df8a282397b5ed1b6117f19babfbe1ed6e","scrolled":true,"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"code","source":"# this section calculates Remaining Useful Life (RUL) in T-minus notation for the training data\n# find the last cycle per unit number\nmax_cycle = train.groupby('UnitNumber')['Cycle'].max().reset_index()\nmax_cycle.columns = ['UnitNumber', 'MaxOfCycle']\n# merge the max cycle back into the original frame\ntrain_merged = train.merge(max_cycle, left_on='UnitNumber', right_on='UnitNumber', how='inner')\n# calculate RUL for each row\nTarget_Remaining_Useful_Life = train_merged[\"MaxOfCycle\"] - train_merged[\"Cycle\"]\ntrain_with_target = train_merged[\"Target_Remaining_Useful_Life\"] = Target_Remaining_Useful_Life\n# remove unnecessary column\ntrain_with_target = train_merged.drop(\"MaxOfCycle\", axis=1)\ntrain_with_target[train_with_target['UnitNumber'] == 1].head(5)","metadata":{"_cell_guid":"9fea4be0-d753-4322-8b39-f7ce3fab277c","_uuid":"01c76d322f7d758e3ff39dab7938834218a9335d","trusted":true},"execution_count":44,"outputs":[]},{"cell_type":"code","source":"# use seaborn to visualize featuresto target (RUL)\nexplore = sns.PairGrid(data=train_with_target.query('UnitNumber < 15') ,\n                 x_vars=target_var,\n                 y_vars=sensor_columns + op_settings_columns,\n                 hue=\"UnitNumber\", size=3, aspect=2.5)\nexplore = explore.map(plt.scatter, alpha=0.5)\nexplore = explore.set(xlim=(400,0))\nexplore = explore.add_legend()","metadata":{"_cell_guid":"74cc7cfc-42d4-4dd2-b1fc-8eadc7792b7a","_uuid":"d8b85ccf6d9f00a0b93db5f52334820d88883b0a","scrolled":true,"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"# operational setting 3 is stable, let's visualize op setting 1 and 2 against some of the most active sensors\ng = sns.pairplot(data=train_with_target.query('UnitNumber < 15'),\n                 x_vars=[\"Op_Setting_1\",\"Op_Setting_2\"],\n                 y_vars=[\"Sensor_2\", \"Sensor_3\", \"Sensor_4\", \"Sensor_7\", \"Sensor_8\", \"Sensor_9\", \"Sensor_11\", \"Sensor_12\", \"Sensor_13\", \"Sensor_14\", \"Sensor_15\", \"Sensor_17\", \"Sensor_20\", \"Sensor_21\"],\n                 hue=\"UnitNumber\", aspect=1)","metadata":{"_cell_guid":"c407b90b-ed69-41b1-b7c7-a29b50effdae","_uuid":"45bd1033bcb3937dcd292317ead4618edb527c23","scrolled":true,"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"# now it's time to clear out target leakage\nprint(train_with_target.shape)\nleakage_to_drop = ['UnitNumber', 'Cycle', 'Op_Setting_1', 'Op_Setting_2', 'Op_Setting_3']  \ntrain_no_leakage = train_with_target.drop(leakage_to_drop, axis = 1)\nprint(train_no_leakage.shape)\n# set up features and target variable \ny = train_no_leakage['Target_Remaining_Useful_Life']\nX = train_no_leakage.drop(['Target_Remaining_Useful_Life'], axis = 1)","metadata":{"_cell_guid":"8461a459-8c7e-4d4d-99eb-814f4d594d09","_uuid":"009cceb345cd1495d738c6ca443cd522f11cd800","trusted":true},"execution_count":47,"outputs":[]},{"cell_type":"code","source":"# I like to use a simple random forest to determine some of the most important/meaningful features. Can be used as feature selection\n# create an exhuastive random forest (200 trees up to 15 levels deep)\nfrom sklearn import ensemble\nrf = ensemble.RandomForestRegressor()\nsingle_rf = ensemble.RandomForestRegressor(n_estimators = 200, max_depth = 15)\nsingle_rf.fit(X, y)\ny_pred = single_rf.predict(X)\nprint(\"complete\")","metadata":{"_cell_guid":"1523a24c-4588-4639-bb0a-0fdb6efbac12","_uuid":"3d54c2e5816285b729106bc7f6fcb167f6479f2e","trusted":true},"execution_count":46,"outputs":[]},{"cell_type":"code","source":"# graph feature importance\nimport matplotlib.pyplot as plt\nimportances = single_rf.feature_importances_\nindices = np.argsort(importances)[::-1]\nfeature_names = X.columns    \nf, ax = plt.subplots(figsize=(11, 9))\nplt.title(\"Feature ranking\", fontsize = 20)\nplt.bar(range(X.shape[1]), importances[indices], color=\"b\", align=\"center\")\nplt.xticks(range(X.shape[1]), indices) #feature_names, rotation='vertical')\nplt.xlim([-1, X.shape[1]])\nplt.ylabel(\"importance\", fontsize = 18)\nplt.xlabel(\"index of the feature\", fontsize = 18)\nplt.show()\n# list feature importance\nimportant_features = pd.Series(data=single_rf.feature_importances_,index=X.columns)\nimportant_features.sort_values(ascending=False,inplace=True)\nprint(important_features.head(10))","metadata":{"_cell_guid":"0ff72524-2c50-4ae5-a231-59d5539de670","_uuid":"c79945e295023fc861fc9fe36872d318ed074944","trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"# based on the graphs as well as random forest feature importance, I will exclude sensors without much valuable information\nprint(train_no_leakage.shape)\nvars_to_drop = [\"Sensor_\"+str(i) for i in [5, 15, 9, 17, 4, 18]]\ntrain_final = train_no_leakage.drop(vars_to_drop, axis = 1)\nprint(train_final.shape)","metadata":{"_cell_guid":"36bb9a5d-f796-45e5-989a-3d2d98408ce3","_uuid":"a5fa869ccac9d23d4b83a937b49f6ff5bdc00050","trusted":true},"execution_count":48,"outputs":[]},{"cell_type":"code","source":"# identify categorical and numeric fields\nfrom sklearn import preprocessing\ncategorical = train_final.select_dtypes(include=['object'])\nnumeric = train_final.select_dtypes(exclude=['object'])\nprint(categorical.columns.values)\n# create dummy variables (if any categorical fields)\nfor name, values in categorical.items():\n    print(name)\n    dummies = pd.get_dummies(values.str.strip(), prefix = name, dummy_na=True)\n    numeric = pd.concat([numeric, dummies], axis=1)\n# imputation (if any NULL values)\nfor name in numeric:\n    print(name)\n    if pd.isnull(numeric[name]).sum() > 0:\n        numeric[\"%s_mi\" % (name)] = pd.isnull(numeric[name])\n        median = numeric[name].median()\n        numeric[name] = numeric[name].apply(lambda x: median if pd.isnull(x) else x)\ny = numeric['Target_Remaining_Useful_Life']\nX = numeric.drop(['Target_Remaining_Useful_Life'], axis = 1)","metadata":{"_cell_guid":"dddd8dd4-08c9-48d7-87b2-284dec6475c3","_uuid":"fe359b75e1244d75b8ffd833e5502eca9e2ce1d5","trusted":true},"execution_count":49,"outputs":[]},{"cell_type":"code","source":"# random forest regression\n# create holdout\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1234)\n# choose the model\nfrom sklearn.ensemble import RandomForestRegressor\nrf = ensemble.RandomForestRegressor()\n# set up 5-fold cross-validation\nfrom sklearn import model_selection\ncv = model_selection.KFold(5)\n# pipeline standardization and model\nfrom sklearn.pipeline import Pipeline\npipeline = Pipeline(steps=[('standardize', preprocessing.StandardScaler())\n                           , ('model', rf) ])\n# tune the model\nmy_min_samples_leaf = [2, 10, 25, 50, 100]\nmy_max_depth = [7, 8, 9, 10, 11, 12]\n# run the model using gridsearch, select the model with best search\nfrom sklearn.model_selection import GridSearchCV\noptimized_rf = GridSearchCV(estimator=pipeline\n                            , cv=cv\n                            , param_grid =dict(model__min_samples_leaf = my_min_samples_leaf, model__max_depth = my_max_depth)\n                            , scoring = 'neg_mean_squared_error'\n                            , verbose = 1\n                            , n_jobs = -1\n                           )\noptimized_rf.fit(X_train, y_train)\n# show the best model estimators\nprint(optimized_rf.best_estimator_)\n# evaluate metrics on holdout\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\ny_pred = optimized_rf.predict(X_test)\nprint(\"Random Forest Mean Squared Error: \", mean_squared_error(y_test, y_pred))\nprint(\"Random Forest Mean Absolute Error: \", mean_absolute_error(y_test, y_pred))\nprint(\"Random Forest r-squared: \", r2_score(y_test, y_pred))","metadata":{"_cell_guid":"71b0bba8-013a-40db-811e-e5666de00cad","_uuid":"b9a765f9d5331b92f53f1e9ed0cec88f3aa5fa88","scrolled":true,"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Elastic Net GLM\n# create holdout\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1234)\n# choose the model\nfrom sklearn.linear_model import ElasticNet\nglm_net = ElasticNet()\n\n# set up 5-fold cross-validation\nfrom sklearn import model_selection\ncv = model_selection.KFold(5)\n# pipeline standardization and model\nfrom sklearn.pipeline import Pipeline\npipeline = Pipeline(steps=[('standardize', preprocessing.StandardScaler())\n                           , ('model', glm_net) ])\n# tune the model\nmy_alpha = np.linspace(.01, 1, num=5)\nmy_l1_ratio = np.linspace(.01, 1, num=3)\n# run the model using gridsearch, select the model with best search\nfrom sklearn.model_selection import GridSearchCV\noptimized_glm_net = GridSearchCV(estimator=pipeline\n                            , cv=cv\n                            , param_grid =dict(model__l1_ratio = my_l1_ratio, model__alpha = my_alpha)\n                            , scoring = 'neg_mean_squared_error'\n                            , verbose = 1\n                            , n_jobs = -1\n                           )\noptimized_glm_net.fit(X_train, y_train)\n# show the best model estimators\nprint(optimized_glm_net.best_estimator_)\n# evaluate metrics on holdout\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\ny_pred = optimized_glm_net.predict(X_test)\nprint(\"GLM Elastic Net Mean Squared Error: \", mean_squared_error(y_test, y_pred))\nprint(\"GLM Elastic Net Mean Absolute Error: \", mean_absolute_error(y_test, y_pred))\nprint(\"GLM Elastic Net r-squared: \", r2_score(y_test, y_pred))","metadata":{"_cell_guid":"6721199c-b182-474e-82f8-01d6e2a7682e","_uuid":"f127055bb9f9834e31de2d19deac190481eddb17","collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Support Vector Machines\n# create holdout\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1234)\n# choose the model\nfrom sklearn import svm\nfrom sklearn.svm import SVR\nsvm = svm.SVR()\n# set up 5-fold cross-validation\nfrom sklearn import model_selection\ncv = model_selection.KFold(5)\n# pipeline standardization and model\nfrom sklearn.pipeline import Pipeline\npipeline = Pipeline(steps=[('standardize', preprocessing.StandardScaler())\n                           , ('model', svm) ])\n# tune the model\nmy_C = [1]\nmy_epsilon = [.05, .1, .15]\n# run the model using gridsearch, select the model with best search\nfrom sklearn.model_selection import GridSearchCV\noptimized_svm = GridSearchCV(estimator=pipeline\n                            , cv=cv\n                            , param_grid =dict(model__C = my_C, model__epsilon = my_epsilon)\n                            , scoring = 'neg_mean_squared_error'\n                            , verbose = 1\n                            , n_jobs = -1\n                           )\noptimized_svm.fit(X_train, y_train)\n# show the best model estimators\nprint(optimized_svm.best_estimator_)\n# evaluate metrics on holdout\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\ny_pred = optimized_svm.predict(X_test)\nprint(\"SVM Mean Squared Error: \", mean_squared_error(y_test, y_pred))\nprint(\"SVM Mean Absolute Error: \", mean_absolute_error(y_test, y_pred))\nprint(\"SVM r-squared: \", r2_score(y_test, y_pred))","metadata":{"_cell_guid":"10b6bbaf-17d3-46c4-a2f7-f563b26d5805","_uuid":"66b7229df3b541fbe0a4b4df61d289a100672442","collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Gradient Boosting\n# create holdout\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1234)\n# choose the model\nfrom sklearn.ensemble import GradientBoostingRegressor\ngb = ensemble.GradientBoostingRegressor()\n# set up 5-fold cross-validation\nfrom sklearn import model_selection\ncv = model_selection.KFold(5)\n# pipeline standardization and model\nfrom sklearn.pipeline import Pipeline\npipeline = Pipeline(steps=[('standardize', preprocessing.StandardScaler())\n                           , ('model', gb) ])\n# tune the model\nmy_alpha = [.5, .75, .9]\nmy_n_estimators= [500]\nmy_learning_rate = [0.005, .01]\nmy_max_depth = [4, 5, 6]\n# run the model using gridsearch, select the model with best search\nfrom sklearn.model_selection import GridSearchCV\noptimized_gb = GridSearchCV(estimator=pipeline\n                            , cv=cv\n                            , param_grid =dict(model__max_depth = my_max_depth, model__n_estimators = my_n_estimators,\n                                              model__learning_rate = my_learning_rate, model__alpha = my_alpha)\n                            , scoring = 'neg_mean_squared_error'\n                            , verbose = 1\n                            , n_jobs = -1\n                           )\noptimized_gb.fit(X_train, y_train)\n# show the best model estimators\nprint(optimized_gb.best_estimator_)\n# evaluate metrics on holdout\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\ny_pred = optimized_gb.predict(X_test)\nprint(\"Gradient Boosting Mean Squared Error: \", mean_squared_error(y_test, y_pred))\nprint(\"Gradient Boosting Mean Absolute Error: \", mean_absolute_error(y_test, y_pred))\nprint(\"Gradient Boosting r-squared: \", r2_score(y_test, y_pred))","metadata":{"_cell_guid":"19c2f594-b6b1-4144-a42d-0c4d3f3ccc6c","_uuid":"ad4f0ae1bd7c1cf81a6e017cb25f507c21b1c4a6","trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"# plot actual vs predicted Remaining Useful Life for the best model (GBM)\nfig, ax = plt.subplots()\nax.scatter(y_test, y_pred, edgecolors=(0, 0, 0))\nax.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=4)\nax.set_xlabel('Actual RUL')\nax.set_ylabel('Predicted RUL')\nax.set_title('Remaining Useful Life Actual vs. Predicted')\nplt.show()","metadata":{"_cell_guid":"07c79a71-0367-46d0-85c4-1d70176eb63e","_uuid":"0f81af9bc736e42c5e6c91ffce45bf39dbf985dc","scrolled":true,"trusted":true},"execution_count":50,"outputs":[]},{"cell_type":"code","source":"# now let's look at turning this into a classification sol -> can we confidently identify when an asset within its last 15 cycles?\n# generate label columns for training data\ncycles = 15\ntrain_no_leakage['Target_15_Cycles'] = np.where(train_no_leakage['Target_Remaining_Useful_Life'] <= cycles, 1, 0 )\ntrain_no_leakage.tail(5)","metadata":{"_uuid":"1936a61a755b499a692f5ead06a18728611e26c9","trusted":true},"execution_count":60,"outputs":[]},{"cell_type":"code","source":"# based on the graphs as well as random forest feature importance, I will exclude sensors without much valuable information\nprint(train_no_leakage.shape)\nvars_to_drop = [\"Sensor_\"+str(i) for i in [5, 15, 9, 17, 4, 18]]\ntarget_to_drop = ['Target_Remaining_Useful_Life']\ntrain_final = train_no_leakage.drop(vars_to_drop, axis = 1)\ntrain_final = train_no_leakage.drop(target_to_drop, axis = 1)\ntrain_final.tail()","metadata":{"_uuid":"47b84dea739604b0cae4e68f5d67e98ec0acee2e","trusted":true},"execution_count":62,"outputs":[]},{"cell_type":"code","source":"# identify categorical and numeric fields\nfrom sklearn import preprocessing\ncategorical = train_final.select_dtypes(include=['object'])\nnumeric = train_final.select_dtypes(exclude=['object'])\nprint(categorical.columns.values)\n# create dummy variables (if any categorical fields)\nfor name, values in categorical.items():\n    print(name)\n    dummies = pd.get_dummies(values.str.strip(), prefix = name, dummy_na=True)\n    numeric = pd.concat([numeric, dummies], axis=1)\n# imputation (if any NULL values)\nfor name in numeric:\n    print(name)\n    if pd.isnull(numeric[name]).sum() > 0:\n        numeric[\"%s_mi\" % (name)] = pd.isnull(numeric[name])\n        median = numeric[name].median()\n        numeric[name] = numeric[name].apply(lambda x: median if pd.isnull(x) else x)\ny = numeric['Target_15_Cycles']\nX = numeric.drop(['Target_15_Cycles'], axis = 1)","metadata":{"_uuid":"12735e1c1ad984d93c01a8420a4f7cb1ec190554","trusted":true},"execution_count":63,"outputs":[]},{"cell_type":"code","source":"# random forest regression\n# create holdout\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1234)\n# choose the model\nfrom sklearn import ensemble\nfrom sklearn.ensemble import RandomForestClassifier\nrf = ensemble.RandomForestClassifier()\n# set up 5-fold cross-validation\nfrom sklearn import model_selection\ncv = model_selection.KFold(5)\n# pipeline standardization and model\nfrom sklearn.pipeline import Pipeline\npipeline = Pipeline(steps=[('standardize', preprocessing.StandardScaler())\n                           , ('model', rf) ])\n# tune the model\nmy_min_samples_leaf = [2, 25, 50]\nmy_max_depth = [8, 9, 10, 12]\n# run the model using gridsearch, select the model with best search\nfrom sklearn.model_selection import GridSearchCV\noptimized_rf = GridSearchCV(estimator=pipeline\n                            , cv=cv\n                            , param_grid =dict(model__min_samples_leaf = my_min_samples_leaf, model__max_depth = my_max_depth)\n                            , scoring = 'roc_auc'\n                            , verbose = 1\n                            , n_jobs = -1\n                           )\noptimized_rf.fit(X_train, y_train)\n# show the best model estimators\ny_pred_proba = optimized_rf.predict_proba(X_test)[:, 1]\ny_pred = optimized_rf.predict(X_test)\nprint(optimized_rf.best_estimator_)","metadata":{"_uuid":"a8e243a3bf318db88e3876661fcf14e504fcccad","trusted":true},"execution_count":65,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import roc_auc_score, accuracy_score, precision_score, recall_score, confusion_matrix\nprint(\"Confusion Matrix:\")\nprint(confusion_matrix(y_test, y_pred))\nfrom sklearn.metrics import classification_report\nprint(\"Random Forest Accuracy: \"+\"{:.1%}\".format(accuracy_score(y_test, y_pred)));\nprint(\"Random Forest Precision: \"+\"{:.1%}\".format(precision_score(y_test, y_pred)));\nprint(\"Random Forest Recall: \"+\"{:.1%}\".format(recall_score(y_test, y_pred)));\nprint(\"Classification Report:\")\nprint(classification_report(y_test, y_pred))\nfrom sklearn import metrics\nfpr, tpr, threshold = metrics.roc_curve(y_test, y_pred_proba)\nroc_auc = metrics.auc(fpr, tpr)\nimport matplotlib.pyplot as plt\nplt.title('Receiver Operating Characteristic')\nplt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\nplt.legend(loc = 'lower right')\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlim([0, 1])\nplt.ylim([0, 1])\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.show()","metadata":{"_uuid":"f4a403ca91740ae9536acd8bcd3d7d996f27d7b7","trusted":true},"execution_count":66,"outputs":[]}]}